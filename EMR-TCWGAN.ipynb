{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.modules[__name__].__dict__.clear()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input,Concatenate,Activation, Dense, Dropout, Flatten, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape\n",
    "from keras.models import Model\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow import keras\n",
    "\n",
    "#Tx: time dimension\n",
    "#nx: number of features\n",
    "\n",
    "# generator\n",
    "def make_generator_model(latent_dim,Tx,nx):\n",
    "    dim1= 2\n",
    "    dim2=int(nx/2)\n",
    "    depth=256\n",
    "\n",
    "    in_label = Input(shape=(1,))\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    merge = Concatenate(axis=1)([in_lat, in_label])\n",
    "\n",
    "    n_nodes = dim1 * dim2 * depth\n",
    "    gen = Dense(n_nodes)(merge)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    gen = LeakyReLU()(gen)\n",
    "    gen = Reshape((dim1, dim2, depth))(gen)\n",
    "\n",
    "\n",
    "    gen = Conv2DTranspose(128,(3,3), strides=(1,1), padding='same')(gen)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    gen = LeakyReLU()(gen)\n",
    "    gen=Dropout(0.2)(gen)\n",
    "\n",
    "    gen = Conv2DTranspose(64,(3,3), strides=(1,1), padding='same')(gen)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    gen = LeakyReLU()(gen)\n",
    "    gen=Dropout(0.2)(gen)\n",
    "\n",
    "\n",
    "    gen = Conv2DTranspose(1,(3,3), strides=(2,2), padding='same')(gen)\n",
    "    #gen = BatchNormalization()(gen)\n",
    "    #gen = LeakyReLU()(gen)\n",
    "    #gen=Dropout(0.5)(gen)\n",
    "    out_layer = Activation('tanh')(gen)\n",
    "    model = Model([in_lat, in_label], out_layer,name='Generator')\n",
    "    #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #gen = Conv2D(1,(3,3), strides=(1,1), padding='same')(gen)\n",
    "    #out_layer = Activation('tanh')(gen)\n",
    "    #model = Model([in_lat, in_label], out_layer)\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#discriminator\n",
    "def make_discriminator_model(Tx,nx):\n",
    "\n",
    "    dim1=Tx\n",
    "    dim2=nx\n",
    "\n",
    "    in_image= Input (shape=[dim1, dim2, 2])\n",
    "    fe = Conv2D(64,(3,3), strides=(2,2), padding='same')(in_image)\n",
    "    fe= LeakyReLU(alpha=0.2)(fe)\n",
    "    fe=Dropout(0.2)(fe)\n",
    "\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2),padding='same')(fe)\n",
    "    #fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.2)(fe)\n",
    "\n",
    "    fe = Conv2D(256,(3,3), strides=(2,2), padding='same')(fe)\n",
    "    #fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.2)(fe)\n",
    "\n",
    "    fe = Conv2D(512,(3,3), strides=(2,2), padding='same')(fe)\n",
    "    #fe = BatchNormalization()(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(0.2)(fe)\n",
    "\n",
    "    fe = Flatten()(fe)\n",
    "    #real/fake output\n",
    "    out1 =  Dense(1,activation='linear')(fe)\n",
    "\n",
    "    model = Model(in_image, out1,name='Critic')\n",
    "\n",
    "    #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# gradient penalty\n",
    "def gradient_penalty(batch_size, crit, real_images, fake_images):\n",
    "    alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    diff = fake_images - real_images\n",
    "    interpolated = real_images + alpha * diff\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = crit(interpolated, training=True)\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "\n",
    "\n",
    "    #gradient = gradient.flatten()\n",
    "    #gradient_norm = tf.norm(gradient, ord=2, axis=1)\n",
    "    #penalty = ((gradient_norm -1)**2).mean()\n",
    "    #return penalty\n",
    "\n",
    "# generator loss\n",
    "def get_gen_loss(crit_fake_pred):\n",
    "    return -tf.reduce_mean(crit_fake_pred)\n",
    "#discriminator loss\n",
    "def get_crit_loss (crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
    "    real_loss = tf.reduce_mean(crit_real_pred)\n",
    "    fake_loss = tf.reduce_mean(crit_fake_pred)\n",
    "    d_cost = fake_loss - real_loss\n",
    "    crit_loss = d_cost + gp*c_lambda\n",
    "    return crit_loss\n",
    "\n",
    "# make one hot images from the ground true labels\n",
    "def one_hot_images(dim,labels):\n",
    "    Lx = np.zeros((dim[0],dim[1],dim[2],1))\n",
    "    for i in range(labels.shape[0]):\n",
    "        if labels[i]== 0:\n",
    "            Lx[i] = np.zeros((1,dim[1],dim[2],1))\n",
    "        elif labels[i]== 1:\n",
    "            Lx[i] = np.ones((1,dim[1],dim[2],1))\n",
    "\n",
    "    #X_new = np.concatenate((X,Lx),axis=3)\n",
    "    return  Lx\n",
    "\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # concatenate the one hot images of labels to the X\n",
    "    dim = X.shape\n",
    "    Lx = one_hot_images(dim,labels)\n",
    "    X = np.concatenate((X,Lx),axis=3)\n",
    "    return X, labels\n",
    "\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=2):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples) #check these labels!\n",
    "\n",
    "\n",
    "    return [z_input, labels]\n",
    "\n",
    "\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, dataset, latent_dim ,gen_loss, disc_loss, n_epochs=700, n_batch=8):\n",
    "\n",
    "\n",
    "    d_optimizer = keras.optimizers.Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.9)\n",
    "    g_optimizer = keras.optimizers.Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.9)\n",
    "\n",
    "    n_steps=5000\n",
    "    c_lambda = 5\n",
    "\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        #train the critic 3 times more than the generator\n",
    "        for j in range(3):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "            # generate fake samples\n",
    "            dim = X_real.shape\n",
    "            z_input, labels_fake = generate_latent_points(latent_dim, n_batch)\n",
    "            Lx = one_hot_images(dim,labels_fake)\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                images = g_model([z_input, labels_fake], training= True)\n",
    "\n",
    "                # concatenate the one hot images of labels to the images\n",
    "                X_fake = np.concatenate((images,Lx),axis=3)\n",
    "\n",
    "                # get the logits for fake images\n",
    "                crit_fake_pred = d_model(X_fake, training= True)\n",
    "                # get the logits for real images\n",
    "                crit_real_pred = d_model(X_real, training= True)\n",
    "\n",
    "                # gradient penalty\n",
    "                gp = gradient_penalty(n_batch, d_model, X_real, X_fake)\n",
    "                d_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, d_model.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            d_optimizer.apply_gradients(zip(d_gradient, d_model.trainable_variables))\n",
    "\n",
    "\n",
    "        # prepare points in latent space as input for the generator\n",
    "        z_input, z_labels = generate_latent_points(latent_dim, n_batch)\n",
    "\n",
    "        dim=[]\n",
    "        dim.append(n_batch)\n",
    "        dim.append(X_real.shape[1])\n",
    "        dim.append(X_real.shape[2])\n",
    "        dim.append(X_real.shape[3])\n",
    "        label_image = one_hot_images(dim,z_labels)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = g_model([z_input, z_labels], training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            Input_new_fake = tf.concat([generated_images, label_image], axis=3)\n",
    "            crit_fake_pred = d_model(Input_new_fake, training=True)\n",
    "            g_loss = get_gen_loss(crit_fake_pred)\n",
    "\n",
    "\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, g_model.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, g_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "        gen_loss.append(g_loss)\n",
    "        disc_loss.append(d_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return gen_loss, disc_loss, g_model, d_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "discriminator = make_discriminator_model(Tx,nx)\n",
    "# create the generator\n",
    "generator = make_generator_model(latent_dim,Tx,nx)\n",
    "\n",
    "# load image data\n",
    "dataset = [X_train, Y_train]  # you need to change this part x_train dimension: (m,Tx,nx) m: number of samples, Tx: time dimension, nx: number of features\n",
    "\n",
    "gen_loss = []\n",
    "disc_loss =[]\n",
    "\n",
    "# train model\n",
    "gen_loss, disc_loss, generator,discriminator = train(generator, discriminator, dataset, latent_dim,gen_loss, disc_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}